{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import typing as t\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define dataset information\n",
    "IMAGE_SHAPE = (3, 224, 224)  # Shape of the input images: (channels, height, width)\n",
    "NUM_CLASSES = 100  # Number of classes in the dataset\n",
    "DATASET_SIZE = 1000  # Size of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_logging(\n",
    "    prename: str = \"log\",\n",
    "    log_level: str = \"INFO\",\n",
    "    log_path: str = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Configures the logging system to save logs at a specified level to a file named with a\n",
    "    prefix provided by the user and the current datetime. The logs can be saved in a specified\n",
    "    directory if `log_path` is provided.\n",
    "\n",
    "    Parameters:\n",
    "    - prename (str): The prefix for the logfile name. Default is \"log\".\n",
    "    - log_level (str): The logging level as a string (e.g., 'INFO', 'DEBUG', 'ERROR').\n",
    "                       Default is \"INFO\".\n",
    "    - log_path (str): The directory path where the log file will be saved. If None,\n",
    "                      the log file will be saved in the current directory. Default is None.\n",
    "\n",
    "    The function creates the directory specified by `log_path` if it does not already exist.\n",
    "    \"\"\"\n",
    "    # Format the current date and time to append to the filename\n",
    "    current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    if log_path:\n",
    "        folder_name, _ = os.path.split(log_path)\n",
    "        if not os.path.exists(folder_name) and folder_name != \"\":\n",
    "            os.makedirs(folder_name)\n",
    "        filename = os.path.join(log_path, f\"{prename}_{current_time}.log\")\n",
    "    else:\n",
    "        filename = f\"{prename}_{current_time}.log\"\n",
    "\n",
    "    # Configure logging\n",
    "    logging.basicConfig(\n",
    "        level=logging.getLevelName(\n",
    "            log_level.upper()\n",
    "        ),  # Ensure log level is correctly interpreted\n",
    "        format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "        filename=filename,\n",
    "        filemode=\"w\",  # Overwrites the file with each run; use 'a' to append\n",
    "    )\n",
    "\n",
    "    logging.info(\"Logging is configured and started.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device_info(device: torch.device) -> str:\n",
    "    \"\"\"\n",
    "    Retrieves information about the specified torch device.\n",
    "\n",
    "    Parameters:\n",
    "    - device: torch.device - The device for which information is being retrieved.\n",
    "\n",
    "    Returns:\n",
    "    - str: A formatted string containing device details.\n",
    "    \"\"\"\n",
    "    if device.type == \"cuda\":\n",
    "        info = torch.cuda.get_device_properties(device)\n",
    "        return f\"Device: {device} (Name: {info.name}, Memory: {info.total_memory / 1e9:.2f} GB)\"\n",
    "    else:\n",
    "        return f\"Device: {device} (CPU)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_size(model: nn.Module) -> str:\n",
    "    \"\"\"\n",
    "    Calculates the total number of trainable parameters in a model.\n",
    "\n",
    "    Parameters:\n",
    "    - model: nn.Module - The model whose parameters are being counted.\n",
    "\n",
    "    Returns:\n",
    "    - str: A formatted string stating the total number of trainable parameters.\n",
    "    \"\"\"\n",
    "    model_parameters = sum(\n",
    "        param.numel() for param in model.parameters() if param.requires_grad\n",
    "    )\n",
    "    return f\"Model Size: {model_parameters} parameters\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimal_batch_size(\n",
    "    model: nn.Module,\n",
    "    device: torch.device,\n",
    "    input_shape: t.Tuple[int, int, int],\n",
    "    output_shape: t.Tuple[int],\n",
    "    dataset_size: int,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    max_batch_size: int = None,\n",
    "    num_iterations: int = 5,\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Determines the optimal batch size for training based on available device memory.\n",
    "\n",
    "    Parameters:\n",
    "    - model: nn.Module - The model to be trained.\n",
    "    - device: torch.device - The device on which the model will be trained.\n",
    "    - input_shape: Tuple[int, int, int] - The shape of the input data.\n",
    "    - output_shape: Tuple[int] - The shape of the output data.\n",
    "    - dataset_size: int - The total size of the dataset.\n",
    "    - optimizer: torch.optim.Optimizer - The optimizer used for training.\n",
    "    - max_batch_size: int (optional) - The maximum allowable batch size.\n",
    "    - num_iterations: int (optional) - The number of iterations to test for memory errors.\n",
    "\n",
    "    Returns:\n",
    "    - int: The determined optimal batch size.\n",
    "    \"\"\"\n",
    "    logging.info(\"Starting batch size determination.\")\n",
    "    logging.info(get_device_info(device))\n",
    "    logging.info(get_model_size(model))\n",
    "\n",
    "    if max_batch_size is not None and max_batch_size <= 0:\n",
    "        logging.error(\"max_batch_size must be a positive integer.\")\n",
    "        raise ValueError(\"max_batch_size must be a positive integer\")\n",
    "    if dataset_size <= 0:\n",
    "        logging.error(\"dataset_size must be a positive integer.\")\n",
    "        raise ValueError(\"dataset_size must be a positive integer\")\n",
    "\n",
    "    batch_size = 2\n",
    "    while True:\n",
    "        logging.info(f\"Testing batch size: {batch_size}\")\n",
    "        if max_batch_size is not None and batch_size > max_batch_size:\n",
    "            batch_size = max_batch_size\n",
    "            logging.info(f\"Reached max_batch_size. Setting batch size to {batch_size}.\")\n",
    "            break\n",
    "        if batch_size > dataset_size:\n",
    "            batch_size = dataset_size\n",
    "            logging.info(\n",
    "                f\"Batch size exceeds dataset size. Setting batch size to {batch_size}.\"\n",
    "            )\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                for _ in range(num_iterations):\n",
    "                    inputs = torch.rand(batch_size, *input_shape, device=device)\n",
    "                    targets = torch.rand(batch_size, *output_shape, device=device)\n",
    "                    optimizer.zero_grad()\n",
    "                    with torch.enable_grad():\n",
    "                        outputs = model(inputs)\n",
    "                        loss = F.mse_loss(outputs, targets)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "            logging.info(\n",
    "                f\"Batch size {batch_size} successful. Doubling batch size for next test.\"\n",
    "            )\n",
    "            batch_size *= 2\n",
    "        except RuntimeError as e:\n",
    "            if \"out of memory\" in str(e):\n",
    "                batch_size = max(2, batch_size // 2)\n",
    "                logging.warning(\n",
    "                    f\"Out of memory error with batch size {batch_size*2}. Halving to {batch_size}.\"\n",
    "                )\n",
    "                break\n",
    "            else:\n",
    "                logging.error(\"Unexpected RuntimeError.\", exc_info=True)\n",
    "                raise e\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    logging.info(f\"Final optimal batch size determined: {batch_size}\")\n",
    "    return batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets(\n",
    "    batch_size: int, num_workers: int = 2\n",
    ") -> t.Tuple[DataLoader, DataLoader]:\n",
    "    \"\"\"\n",
    "    Prepares DataLoader instances for training and testing datasets.\n",
    "\n",
    "    Parameters:\n",
    "    - batch_size: int - The batch size for data loading.\n",
    "    - num_workers: int (optional) - The number of worker processes for data loading.\n",
    "\n",
    "    Returns:\n",
    "    - Tuple[DataLoader, DataLoader]: A tuple containing the training and testing DataLoaders.\n",
    "    \"\"\"\n",
    "    train_ds = DataLoader(\n",
    "        datasets.FakeData(\n",
    "            size=DATASET_SIZE,\n",
    "            image_size=IMAGE_SHAPE,\n",
    "            num_classes=NUM_CLASSES,\n",
    "            transform=transforms.Compose([transforms.ToTensor()]),\n",
    "        ),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "    test_ds = DataLoader(\n",
    "        datasets.FakeData(\n",
    "            size=200,\n",
    "            image_size=IMAGE_SHAPE,\n",
    "            num_classes=NUM_CLASSES,\n",
    "            transform=transforms.Compose([transforms.ToTensor()]),\n",
    "        ),\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "    return train_ds, test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    \"\"\"\n",
    "    A modified ResNet model for classification.\n",
    "\n",
    "    Inherits from nn.Module and integrates a pretrained ResNet50 model with a custom output layer for classification.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.resnet = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.GELU(),\n",
    "            nn.Linear(in_features=1000, out_features=NUM_CLASSES),\n",
    "            nn.LogSoftmax(dim=-1),\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        Parameters:\n",
    "        - inputs: torch.Tensor - The input data.\n",
    "\n",
    "        Returns:\n",
    "        - torch.Tensor: The model's output.\n",
    "        \"\"\"\n",
    "        outputs = self.resnet(inputs)\n",
    "        outputs = self.output_layer(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    train_ds: DataLoader,\n",
    "    device: torch.device,\n",
    ") -> t.Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Trains the model on the training dataset for one epoch.\n",
    "\n",
    "    Parameters:\n",
    "    - model: nn.Module - The model to be trained.\n",
    "    - optimizer: torch.optim.Optimizer - The optimizer for training.\n",
    "    - train_ds: DataLoader - The DataLoader for the training data.\n",
    "    - device: torch.device - The device on which to perform training.\n",
    "\n",
    "    Returns:\n",
    "    - Dict[str, float]: A dictionary containing the average loss and accuracy for the training epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    train_loss, correct = 0, 0\n",
    "    for _, (data, target) in enumerate(tqdm(train_ds, desc=\"Train\")):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        pred = output.argmax(\n",
    "            dim=1, keepdim=True\n",
    "        )  # Simplified from max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    return {\n",
    "        \"loss\": train_loss\n",
    "        / len(train_ds.dataset),  # Corrected to divide by dataset size for average\n",
    "        \"accuracy\": 100.0 * correct / len(train_ds.dataset),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(\n",
    "    model: nn.Module,\n",
    "    test_ds: DataLoader,\n",
    "    device: torch.device,\n",
    ") -> t.Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Evaluates the model on the testing dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - model: nn.Module - The model to be evaluated.\n",
    "    - test_ds: DataLoader - The DataLoader for the testing data.\n",
    "    - device: torch.device - The device on which to perform evaluation.\n",
    "\n",
    "    Returns:\n",
    "    - Dict[str, float]: A dictionary containing the average loss and accuracy for the testing data.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in tqdm(test_ds, desc=\"Test\"):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    return {\n",
    "        \"loss\": test_loss\n",
    "        / len(test_ds.dataset),  # Corrected to divide by dataset size for average\n",
    "        \"accuracy\": 100.0 * correct / len(test_ds.dataset),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(epochs: int = 2):\n",
    "    \"\"\"\n",
    "    The main function to execute the training and testing of the model.\n",
    "\n",
    "    Parameters:\n",
    "    - epochs: int (optional) - The number of epochs to train the model.\n",
    "    \"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        raise RuntimeError(\"CUDA is not available.\")\n",
    "\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    model = ResNet().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    configure_logging(\"resnet50_training\")\n",
    "\n",
    "    batch_size = get_optimal_batch_size(\n",
    "        model=model,\n",
    "        device=device,\n",
    "        input_shape=IMAGE_SHAPE,\n",
    "        output_shape=(NUM_CLASSES,),\n",
    "        dataset_size=DATASET_SIZE,\n",
    "        optimizer=optimizer,\n",
    "    )\n",
    "\n",
    "    train_ds, test_ds = get_datasets(batch_size=batch_size)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(f\"\\nEpoch {epoch}/{epochs}\")\n",
    "        train_result = train(\n",
    "            model=model, optimizer=optimizer, train_ds=train_ds, device=device\n",
    "        )\n",
    "        test_result = test(model=model, test_ds=test_ds, device=device)\n",
    "        print(\n",
    "            f'Train loss: {train_result[\"loss\"]:.04f}\\t'\n",
    "            f'accuracy: {train_result[\"accuracy\"]:.2f}%\\n'\n",
    "            f'Test loss: {test_result[\"loss\"]:.04f}\\t'\n",
    "            f'accuracy: {test_result[\"accuracy\"]:.2f}%'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
